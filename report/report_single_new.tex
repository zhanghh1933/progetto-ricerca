\chapter{Single Scenario Classification, KFold Validation}Starting with fitting randomly the classifiers, there are some statistics of the data used for the first test: \\
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l|l|l|} 
\hline 
  &count train  &count test  \\ \hline
messenger  &249  &100  \\ \hline
telegram  &244  &106  \\ \hline
whatsapp  &243  &107  \\ \hline
original  &243  &107  \\ \hline
\end{tabular} 
\end{table} }
\section{Logistic regression results:} 
Confusion matrix with number of sample and with normalization:
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l|l|l|l|l|} 
\hline 
  &messenger  &telegram  &whatsapp  &original  \\ \hline
messenger  &100  &0  &0  &0  \\ \hline
telegram  &0  &106  &0  &0  \\ \hline
whatsapp  &0  &0  &103  &4  \\ \hline
original  &0  &0  &0  &107  \\ \hline
\end{tabular} 
\end{table} }

 \begin{figure}[H] 
\centering 
\includegraphics[scale=.6]{images/new_met_lr_initial.png} 
\caption{logistic regression} 
\end{figure} 


Result of the KFold validation with 10 bins:
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l |l |l |l |l |l |l |l |l |l |}  
\hline 
0.9796&
0.9898&
1.0000&
1.0000&
1.0000&
0.9898&
0.9898&
1.0000&
0.9898&
1.0000\\ \hline  

\end{tabular} 
\end{table} }

The mean is : 0.993878\section{Linear Support Vector Machine results:}Confusion matrix with number of sample and with normalization:
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l|l|l|l|l|} 
\hline 
  &messenger  &telegram  &whatsapp  &original  \\ \hline
messenger  &100  &0  &0  &0  \\ \hline
telegram  &0  &106  &0  &0  \\ \hline
whatsapp  &0  &0  &103  &4  \\ \hline
original  &0  &0  &0  &107  \\ \hline
\end{tabular} 
\end{table} }

 \begin{figure}[H] 
\centering 
\includegraphics[scale=.6]{images/new_met_lsvm_initial.png} 
\caption{linear SVM} 
\end{figure} 


Result of the KFold validation with 10 bins:
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l |l |l |l |l |l |l |l |l |l |}  
\hline 
0.9898&
0.9898&
1.0000&
1.0000&
1.0000&
0.9796&
0.9898&
1.0000&
0.9898&
1.0000\\ \hline  

\end{tabular} 
\end{table} }

The mean is : 0.993878\section{Random forest results:}Confusion matrix with number of sample and with normalization:
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l|l|l|l|l|} 
\hline 
  &messenger  &telegram  &whatsapp  &original  \\ \hline
messenger  &100  &0  &0  &0  \\ \hline
telegram  &0  &106  &0  &0  \\ \hline
whatsapp  &0  &0  &103  &4  \\ \hline
original  &0  &0  &0  &107  \\ \hline
\end{tabular} 
\end{table} }

 \begin{figure}[H] 
\centering 
\includegraphics[scale=.6]{images/new_met_rf_initial.png} 
\caption{random forest} 
\end{figure} 


Result of the KFold validation with 10 bins:
 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l |l |l |l |l |l |l |l |l |l |}  
\hline 
1.0000&
0.9898&
1.0000&
1.0000&
0.9898&
0.9796&
0.9898&
1.0000&
0.9898&
0.9897\\ \hline  

\end{tabular} 
\end{table} }

The mean is : 0.992847

\chapter{Single Scenario Classification, Circularly Validation}

Here was used the same dataset as before but the training used a 0.3 of the dataset, and it is shifted circulary to cover all the dataset.Here is the table of all steps calculated \\
\begin{longtable} 
{|l |l |l |l |} 
\hline 
step  &logistic  &linear SVM  &random fo.  \\ \hline
0  &0.989179818268771  &0.9861800141743444  &0.9919532891139332  \\ \hline
1  &0.9848642886352729  &0.989179818268771  &0.9898788133362098  \\ \hline
2  &0.9859958494970797  &0.989179818268771  &0.9930034962192048  \\ \hline
3  &0.9840239043824701  &0.989179818268771  &0.98602505332677  \\ \hline
4  &0.9838346881813623  &0.9881717869333969  &0.9868519340790709  \\ \hline
5  &0.9850775862706985  &0.9888435941925785  &0.9900141740496509  \\ \hline
6  &0.9849757394589151  &0.9899004065040651  &0.9929795918367347  \\ \hline
7  &0.9869068386254386  &0.9899004065040651  &0.99103468547913  \\ \hline
8  &0.9859149679167976  &0.9899004065040651  &0.98705352838109  \\ \hline
9  &0.9826572092251535  &0.9861966137690223  &0.9826773061122597  \\ \hline
10  &0.9847290722474007  &0.9870545842786601  &0.9807610095111248  \\ \hline
11  &0.9837185571115683  &0.9857771334299855  &0.9846840787373938  \\ \hline
12  &0.9838354913678619  &0.9859658778205833  &0.9837128120847991  \\ \hline
13  &0.9813181579293129  &0.9861166500498505  &0.9797344992213752  \\ \hline
14  &0.9813181579293129  &0.9851770279255319  &0.9816976401921237  \\ \hline
15  &0.9831900496861925  &0.9862857095347368  &0.9798536585365853  \\ \hline
16  &0.9841168266469769  &0.9862857095347368  &0.9827641540487163  \\ \hline
17  &0.9822572998070824  &0.9822572998070824  &0.97795683313976  \\ \hline
18  &0.9821251322105606  &0.9813315670689824  &0.9769817171132961  \\ \hline
19  &0.9820101172758178  &0.982107843137255  &0.9750549818320903  \\ \hline
20  &0.9820101172758178  &0.9826435137223949  &0.9760144649257553  \\ \hline
21  &0.9820101172758178  &0.9822440033492588  &0.9760144649257553  \\ \hline
22  &0.9820101172758178  &0.9819674282059272  &0.9760144649257553  \\ \hline
23  &0.9789859263543474  &0.9833374567046016  &0.9734258819806992  \\ \hline
24  &0.9789859263543474  &0.9844528594528594  &0.9734258819806992  \\ \hline
25  &0.9790240688968155  &0.9835470085470086  &0.9734258819806992  \\ \hline
26  &0.978963179539905  &0.9808615772912023  &0.9734258819806992  \\ \hline
27  &0.981011696187139  &0.9881608339538348  &0.981094861660079  \\ \hline
28  &0.9809466587092924  &0.9880438882784184  &0.981094861660079  \\ \hline
29  &0.978957428886153  &0.9869016393442622  &0.981094861660079  \\ \hline
30  &0.9771308523409363  &0.9880438882784184  &0.981094861660079  \\ \hline
31  &0.9839638554216867  &0.9889326989562411  &0.981094861660079  \\ \hline
32  &0.9821736011477762  &0.9839576074332173  &0.9725526315789474  \\ \hline
33  &0.9632234670976825  &0.963381121890158  &0.9569935841353164  \\ \hline
34  &0.955915762290795  &0.9604524917457968  &0.9523383383383384  \\ \hline
35  &0.9558080031175651  &0.9615025224051383  &0.9332107165025093  \\ \hline
36  &0.9537713472485769  &0.9615025224051383  &0.9342712270274949  \\ \hline
37  &0.9567246849068246  &0.9695545748887021  &0.941807112194959  \\ \hline
38  &0.9624805441127516  &0.9689582071471836  &0.941807112194959  \\ \hline
39  &0.9656916766799837  &0.9754108565737052  &0.9435515300577979  \\ \hline
40  &0.9645393196105017  &0.9744245524296675  &0.9435515300577979  \\ \hline
41  &0.9674626293689195  &0.9725627105089125  &0.9426760297719203  \\ \hline
42  &0.9654192933722927  &0.970744883788362  &0.9426760297719203  \\ \hline
43  &0.9695591349062311  &0.9723367392625123  &0.9713033424446343  \\ \hline
44  &0.9684887580521552  &0.9724221573471613  &0.9713033424446343  \\ \hline
45  &0.968972132612202  &0.9742295202245372  &0.9680824885108373  \\ \hline
46  &0.9682197824252712  &0.9742295202245372  &0.9735226067675696  \\ \hline
47  &0.9693788613812181  &0.9731363489522036  &0.9735226067675696  \\ \hline
48  &0.9668187320808225  &0.9683336860555347  &0.9724089271961905  \\ \hline
49  &0.9642240738507779  &0.9642997792344016  &0.9651904231493449  \\ \hline
50  &0.9629520363275152  &0.9642997792344016  &0.9598819990386256  \\ \hline
51  &0.9631771897864273  &0.9642997792344016  &0.9651904231493449  \\ \hline
52  &0.9643385011275081  &0.9642997792344016  &0.9631028529724224  \\ \hline
53  &0.9738195798137318  &0.9736723400676142  &0.97138326585695  \\ \hline
54  &0.9782388663967612  &0.9752631578947368  &0.9778754788737738  \\ \hline
55  &0.9782388663967612  &0.9760314801400458  &0.977891836985964  \\ \hline
56  &0.9782388663967612  &0.9676185619581846  &0.9800443458980044  \\ \hline
57  &0.9789586940956656  &0.9722738077551627  &0.9778754788737738  \\ \hline
58  &0.9808488835137682  &0.9799832842596  &0.9789560728306903  \\ \hline
59  &0.9809913155949741  &0.978873669583276  &0.9780137313157126  \\ \hline
60  &0.9820075757575757  &0.9798655402919171  &0.973689373974726  \\ \hline
61  &0.9820075757575757  &0.9828897338403042  &0.9800443458980044  \\ \hline
62  &0.9820075757575757  &0.9820075757575757  &0.9780137313157126  \\ \hline
63  &1.0  &1.0  &0.995  \\ \hline
64  &0.9949551291586097  &0.992013888888889  &0.9889040114113886  \\ \hline
65  &0.9901960784313726  &0.9892578125  &0.9881083866601034  \\ \hline
66  &0.9901960784313726  &0.9892578125  &0.9890582542641577  \\ \hline
67  &0.9901960784313726  &0.9892578125  &0.9891756702681073  \\ \hline
68  &0.9881717869333969  &0.9852417482429718  &0.9862299606105536  \\ \hline
69  &0.9881717869333969  &0.9881717869333969  &0.9901086744322249  \\ \hline
\end{longtable} 
Average of all steps: 

 {\def\arraystretch{1.3} 
 \begin{table}[H] 
\centering 
\begin{tabular}{|l|l|l|} 
\hline 
logistic r.  &linear SVM  &random f.  \\ \hline
0.9777519138070937  &0.9801449719341919  &0.974445885155639  \\ \hline
\end{tabular} 
\end{table} }
Confusion matrix estimated on overall tests: 

 \begin{figure}[H] 
\centering 
\includegraphics[scale=.6]{images/new_met_logistic_total.png} 
\caption{logistic regression} 
\end{figure} 

 \begin{figure}[H] 
\centering 
\includegraphics[scale=.6]{images/new_met_lsvm_total.png} 
\caption{linear SVM} 
\end{figure} 

 \begin{figure}[H] 
\centering 
\includegraphics[scale=.6]{images/new_met_random_total.png} 
\caption{random forest} 
\end{figure} 
